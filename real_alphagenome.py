import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from tqdm import tqdm

# --- 1. Numpy ë°ì´í„°ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ì¡´ì„± ì œê±°ë¨) ---
class NumpyGenomicDataset(Dataset):
    def __init__(self, seq_path, signal_path, length=8192, num_samples=2000):
        """
        ì´ì œ ë³µì¡í•œ íŒŒì‹± ì—†ì´ Numpy ë°°ì—´ì„ ì§ì ‘ ë¡œë“œí•©ë‹ˆë‹¤.
        ì†ë„ê°€ í›¨ì”¬ ë¹ ë¥´ê³ , ìœˆë„ìš°/ë§¥/ë¦¬ëˆ…ìŠ¤ ì–´ë””ì„œë“  ëŒì•„ê°‘ë‹ˆë‹¤.
        """
        print(f"ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘... ({seq_path})")
        # ë©”ëª¨ë¦¬ ë§µ(mmap) ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ RAMì„ ì•„ë‚ë‹ˆë‹¤.
        self.seq_data = np.load(seq_path, mmap_mode='r') 
        self.signal_data = np.load(signal_path, mmap_mode='r')
        
        self.chrom_len = len(self.seq_data)
        self.length = length
        self.num_samples = num_samples
        print("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        # 1. ëœë¤ ìœ„ì¹˜ ì„ íƒ
        start = np.random.randint(0, self.chrom_len - self.length)
        end = start + self.length
        
        # 2. Sequence ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ ìˆ«ìë¡œ ë³€í™˜ë˜ì–´ ìˆìŒ)
        seq_int = self.seq_data[start:end]
        # One-hot Encoding (ì¦‰ì„ ë³€í™˜)
        # 0->A, 1->C, 2->G, 3->T, 4->N
        seq_tensor = torch.zeros(4, self.length, dtype=torch.float32)
        
        # ë²¡í„°í™”ëœ ì—°ì‚°ìœ¼ë¡œ ê³ ì† ì²˜ë¦¬
        seq_tensor[0, seq_int == 0] = 1.0 # A
        seq_tensor[1, seq_int == 1] = 1.0 # C
        seq_tensor[2, seq_int == 2] = 1.0 # G
        seq_tensor[3, seq_int == 3] = 1.0 # T
        
        # 3. Signal ê°€ì ¸ì˜¤ê¸°
        signal_val = self.signal_data[start:end]
        signal_tensor = torch.tensor(signal_val, dtype=torch.float32).unsqueeze(0)
        
        return seq_tensor, signal_tensor

# --- 2. ëª¨ë¸ ì•„í‚¤í…ì²˜ (ì´ì „ê³¼ ë™ì¼) ---
class ResBlock(nn.Module):
    def __init__(self, channels, kernel_size=5, dilation=1):
        super().__init__()
        self.conv1 = nn.Conv1d(channels, channels, kernel_size, padding='same', dilation=dilation)
        self.bn1 = nn.BatchNorm1d(channels)
        self.conv2 = nn.Conv1d(channels, channels, kernel_size, padding='same', dilation=dilation)
        self.bn2 = nn.BatchNorm1d(channels)
        self.gelu = nn.GELU()

    def forward(self, x):
        residual = x
        out = self.gelu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        return self.gelu(out + residual)

class AlphaGenomeMedium(nn.Module):
    def __init__(self, input_len=32768, num_tracks=1):
        super().__init__()
        self.stem = nn.Sequential(nn.Conv1d(4, 128, kernel_size=15, padding=7), nn.GELU(), nn.BatchNorm1d(128))
        self.res_blocks = nn.Sequential(
            ResBlock(128, dilation=1), nn.MaxPool1d(2),
            ResBlock(128, dilation=2), nn.MaxPool1d(2),
            ResBlock(128, dilation=4), nn.MaxPool1d(2),
        )
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=128, nhead=4, batch_first=True, dim_feedforward=512), num_layers=2
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=4), nn.GELU(),
            nn.ConvTranspose1d(64, 32, kernel_size=2, stride=2), nn.GELU(),
            nn.Conv1d(32, num_tracks, kernel_size=1)
        )

    def forward(self, x):
        x = self.stem(x)
        x = self.res_blocks(x)
        x = x.permute(0, 2, 1) 
        x = self.transformer(x)
        x = x.permute(0, 2, 1)
        return self.decoder(x)

# --- 3. ì‹¤í–‰ í•¨ìˆ˜ ---
def train_numpy_model():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ ì¥ì¹˜ ì„¤ì •: {device}")

    # Colabì—ì„œ ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼ ê²½ë¡œ
    SEQ_PATH = "train_seq.npy"
    SIGNAL_PATH = "train_signal.npy"
    
    try:
        # ë°ì´í„°ì…‹ ì´ˆê¸°í™” (ì‹¤ì œ ë°ì´í„°)
        ds = NumpyGenomicDataset(SEQ_PATH, SIGNAL_PATH, num_samples=2000)
        loader = DataLoader(ds, batch_size=4, shuffle=True)
        
        model = AlphaGenomeMedium().to(device)
        optimizer = optim.AdamW(model.parameters(), lr=1e-3)
        criterion = nn.MSELoss()
        
        print("--- í•™ìŠµ ì‹œì‘ ---")
        model.train()
        for epoch in range(5):
            with tqdm(loader, unit="batch", desc=f"Ep {epoch+1}") as tepoch:
                for seq, target in tepoch:
                    seq, target = seq.to(device), target.to(device)
                    
                    optimizer.zero_grad()
                    output = model(seq)
                    loss = criterion(output, target)
                    loss.backward()
                    optimizer.step()
                    
                    tepoch.set_postfix(loss=loss.item())
        print("ğŸ‰ í•™ìŠµ ì™„ë£Œ! ì €ì¥ëœ ëª¨ë¸: best_alphagenome.pth")
        torch.save(model.state_dict(), "best_alphagenome.pth")
        
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '{SEQ_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   1ë‹¨ê³„(Colab)ë¥¼ ì‹¤í–‰í•˜ì—¬ .npy íŒŒì¼ì„ ë¨¼ì € ìƒì„±í•˜ê³  ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    train_numpy_model()